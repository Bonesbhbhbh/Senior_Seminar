%%
%% This is file `sample-sigplan.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigplan')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigplan.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\DocumentMetadata{
  lang=en,
  pdfversion=2.0,
  %pdfstandard=ua-2,
  %testphase={phase-III,firstaid,math,title}
  tagging=on,
  testphase={phase-III,firstaid,math,title}
  %tagging-setup={math/setup=mathml-SE}
}
\documentclass[sigplan,screen,nonacm]{acmart-tagged}
\usepackage{color}
\setlength {\marginparwidth }{2cm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen,review]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{Challenges of Optical Character Recognition}
\author{Orville ``El'' Anderson}
\email{and10393@umn.edu}
\affiliation{
  \institution{Division of Science and Mathematics 
	\\
        University of Minnesota, Morris
	}
  \city{Morris}
  \state{Minnesota}
  \country{USA}
  \postcode{56267}
}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Optical Character Recognition (OCR) is technology used to extract text from images.
OCR has a wide variety of uses, with one common application being to digitize scanned documents.
OCR has three main categories of challenges that reduce accuracy when applied to scanned documents, stemming from page layouts, the writing system used, and visual noise.
By intentionally expanding the documents used to train modern OCR models, it is possible to increase the range of capabilities of this technology.
By increasing the number of document collections that cover these challenges, we make it easier to develop OCR techniques to target them.
This paper looks at document collections for evaluating the extent of these weaknesses and researching methods to address them.
These collections have the added advantage that they can be used to train future OCR models.
\end{abstract}

\doclicenseThis

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{optical character recognition, scanned documents, layout, languages, visual noise, datasets}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{introduction}

Physical documents are used frequently and in a variety of ways.
Some examples are pages of notes, tax forms, and recipes.
A common way to save and share these documents is to photograph them.
This creates a \textit{scanned document}.
The text on the original document may still be significant for users of the scanned document, for purposes such as research and record keeping.
Scanned documents do not contain information about the contents of the physical document.
While text on a scanned document may be readable visually, the contents are not inherently machine-readable.
As a consequence, it is not possible to edit or search within the text of a scanned document.
One method to digitize a scanned document is to manually read and re-type the contents.
An alternative method is to use an \textit{Optical Character Recognition (OCR) model}.
OCR models are programs specifically made to identify text from images.

OCR began as a tool to read messages for Blind people.
The technology grew dramatically in popularity because it could be applied in corporate settings to reduce the human labor needed for data entry~\citep{Schantz:1982}.
As applications for OCR have grown, the technology has been applied to increasingly diverse types of documents.
Several aspects of documents play a role in the effectiveness of OCR on them.
There is an increased need for OCR that works on a wide variety of documents.
One of the big challenges to OCR accuracy is the writing system of the language of the document.
As the third most used writing system, this paper uses examples of Arabic documents to discuss this topic. 
The other two challenges, page layout and visual noise are covered, but to a lesser degree.

%This paper looks at the stages of OCR and some of the emerging Techniques for addressing document variation.
Section~\ref{background} of this paper, covers the stages and techniques used in OCR.
The three challenges related to document variations are the page layout, the visual noise of the image, and the writing system used.
Section~\ref{challenges}, Challenges, looks at how layout, noise, and writing system specifically impact OCR.
It covers how these challenges relate to the stages of OCR and covers some additional steps which can be used to minimize the effect.
Section~\ref{datasets}, goes deeper into the techniques covered in Sections~\ref{background} and Section~\ref{challenges}, specifically into the datasets that were used to research these areas. 
Section~\ref{conclusion}, Conclusion, discusses the importance of specialized datasets and their role in increasing accuracy of OCR.

\section{Stages and Techniques}
\label{background}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{stages.png}
\caption{Stages of OCR pictured on a Persian Document\cite{Fateh:2024}.}
\Description{Some description}
\label{fig:stages}
\end{figure*}

The input and output of the OCR process depends on the model used. 
In general, the input is an image and the output is a text file.
Some models, in addition to extracting text from the image will record the location of where the text was found to construct more complex output formats.
Tesseract, an open-source model discussed in this paper, accepts a variety of file formats, mainly, PNG, JPEG, and TIFF files.
Tesseract currently can format the output as plain text, HTML, as several types of PDFs, and more~\citep{tesseract:2025}.

The process of OCR can be split into a number of stages, but for the purpose of this paper is made of three: \textit{Document Layout Analysis (DLA)}, \textit{Text Line Detection (TLD)}, and \textit{Recognition}.
Two common stages which are not recognized in this list are a pre-processing and post-processing stage.
Because these stages are not necessary to perform OCR, they are instead mentioned in Section~\ref{sec:noise} as noise-reduction techniques.

Figure~\ref{fig:stages} shows a document, in Persian, going through each of the three stages of OCR, where the output of the model is the text identified from the document.
%This is generally returned as plain text, but can include meta-information, such as the location on the source image the text came from~\footnote{Modern OCR models can use this meta-information and text to construct complex output formats like HTML files and can be used in combination with a graphic user interface to display the text output to highlight any potentially incorrectly identified characters.}.
The first stage, DLA, breaks the image up into regions of text.
For the example document, this is two paragraphs, a page number, and some isolated words.
The second stage, TLD, breaks down the sections of text into individual lines of text.
Some TLD methods will further break the lines of text down into into individual words or characters.
The final stage, Recognition, identifies the text in each line and combines the results into one search-able text document.

\subsection{Neural Networks}
\label{sec:NNs}

One technique which can be used in each of the three OCR stages, is to use \textit{Neural Networks}.
Neural Networks are a subsection of Machine Learning meant for pattern recognition.
These networks use large functions with many parameters to make predictions.
In the context of OCR and scanned documents, these predictions are often if a region contains text, or are about the value of some text.

To chose the parameters, or \textit{weights}, in the Neural Network, it must go through a training process.
This process begins with a large collection of examples, \textit{a dataset}.
Datasets can be anything from full documents to individual characters, depending on which stage the Neural Network will be used for.
It is important for the training process that there is am existing machine-readable copy of the contents of the examples.
The dataset is divided into two or more groups.
The Neural Network is exposed to the first group of examples, to get a sense of patterns within the possible outcomes.
In this step the Neural Network will set initial values for the weights.
After exposure, the network is then exposed to the second group of examples.
The network is meant to use the weights chosen in the earlier step to predict the values of the second set.
The accuracy of the predictions are then used to adjust the weights in a way that minimizes errors.
This process repeats as needed.
If the dataset was divided up into more than two groups, the remaining examples can be used as a final measure for accuracy of the Neural Network's predictions.

%A Neural Network takes the pixels of an image and maps them to a series of \textit{nodes}.
%These nodes represent a possible pattern.
%As the input pixels are mapped to the nodes, they are adjusted with a \textit{weight}, to assign a value. [assign a value to what?]
%In the context of images, one possible representation of the input pixels can be a RGB value of the pixel. [bad example]
%Weights are a series of values independent to the input image. 
%At each node, the value of the input is modified with an \textit{activation function}.
%A Neural Network can contain many layers of nodes.
%An activation function is used at each layer to introduce complexity and further control the significance of certain patterns.
%%% Neural Networks will add layers of nodes and activation functions to increase the complexity of patterns it can be applied to.
%
%To determine the weights, Neural networks going through a process called \textit{training}.
%In this process, the neural Network is applied to a variety of documents with known content, a \textit{dataset}.
%The output of the OCR stage is judged and the weights are adjusted to minimize error. 
%Some examples of error in the the context of documents, could be part of the background that was marked as text, or could be a misidentified character.

\subsection{Document Layout Analysis}
\label{sec:DLA}
The purpose of DLA is to identify what part of the input image is text and what is not.
To do this, a model can use a variety of techniques to draw rectangular boxes around the text.
These boxes can be a variety of sizes and can contain varying amounts of text.
% A region can be many things, such as an isolated character, full paragraphs, or tables.
The areas not captured by the boxes are ignored for the rest of the stages, to reduce the workload.

An important step of DLA is preserving the reading order of the document, which can be non-obvious when it comes to documents beyond a single column of text.
Features like the number of columns of text, and the presence of components like images and tables, disrupt a traditional top-down reading order.

Fateh et al~\citep{Fateh:2024} put forward a DLA method which used a voting system on the output of four Neural Networks trained to recognize regions of text.
Each of the four voting Neural Networks, used coordinates to record where on the original image the text was found. 
The overall OCR model they used, Tesseract, takes full lines of text and returns the text as text boxes placed over the original image. 
The need to preserve the document order is hidden when OCR is used on full lines of text and placed in this way.
DLA becomes apparent when the model is used on individual characters or when the output is constrained to one column, like a plain text file.

\subsection{Text Line Detection}
\label{sec:TLD}
TLD takes the previously identified regions of text and further breaks them down into lines, words, or individual characters.
The output of TLD, is each identified unit of text in its own defined box of pixels.
To best fit the text into the boxes, TLD may rotate, center or scale the text.
Traditionally, these boxes have a uniform pre-set size.
It is important for the boxes in question to be large enough to contain the intended character(s), but not to include characters from the neighboring lines or words. 
This results in a long discussion of what is the proper size box.

The size of the unit of text is determined by the technique used in the Recognition stage~\ref{sec:Recognition}.
Modern techniques, such as Neural Networks~\ref{sec:NNs}, can be used on full lines of text, instead of individual characters.
By ending the segmentation process at lines of text, instead of further breaking the text into individual characters, we can prevent introducing errors related to character overlap.
One downside of using full lines of text, is that because they cover more of a page, the line has a larger capacity to be curved.

Fateh et al~\citep{Fateh:2024} propose a TLD technique which uses font size to rotate and standardize full lines of text.
This can be seen in Figure~\ref{fig:stages}, where the text entering the TLD step is at an angle, but the output is rotated, to create a baseline.
By creating this baseline, and somewhat standardizing the characters, the accuracy when defining the characters is improved~\cite{Fateh:2024}.
% Two methods employed by Fateh et al\citep{Fateh:2024} used font size and correcting line rotation, to improve overall accuracy.
To develop this technique, they built a special dataset made to include curved lines of text and lines with very little space in-between. 

\subsection{Recognition}
\label{sec:Recognition}
The final step in the OCR process is to attempt to recognize the text identified in the document.
Because OCR was originally developed to convert printed characters to sound, many of the initial recognition techniques are no longer used.
When the technology moved from noise to text output, some techniques arose which are still relevant.
One such technique is known by many names, one being Matrix Matching.
Matrix Matching is important to understand because it is the foundation of many modern recognition techniques and can help explain the common restrictions in recognizing text.
 
Matrix Matching uses templates of known characters.
In this process, a single unknown character is compared to all of the templates.
For each template, the number of non-matching pixels between it and the single character are recorded.
The output of Matrix Matching is a list of the templates and how similar they were to the unknown character. 
The identity of the template with the highest similarity, is chosen as the identity of the unknown character.

Neural Networks are a popular technique in the Recognition stage.
The networks work similarly to Matrix Matching, but the weights add a level of flexibility when it comes to identifying characters.
Neural Networks are more accommodating to types of fonts and variations in character rotation and placement.
Because of the training process for Neural Networks, through repeated exposure, it is also possible to train the model to predict previously unknown characters.

% There are a lot of variations in classification techniques, but the key thing to note is that all methods use reference material as a basis to classify characters~\cite{Thorat:2022}.
% The output of this recognition step is inherently limited to characters the model has been trained on.

\section{Challenges}
\label{challenges}

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{noise.png}
\caption{Examples of visual noise: Blur, Weak Ink, Salt and Pepper, Scribbles, Ink Stains, and Watermarks.}
\label{fig:noise}
\Description{Six white boxes with the words "sample text" and a type of visual noise applied.}
\end{figure*}

There are three main categories of issues that decrease accuracy of OCR for documents.
These categories are the layout of the original document, the presence of visual noise, and the writing system used.
% Each of these challenges tie back to the key concept that OCR models work best when applied to what they were made to recognize.

\subsection{Layout}
\label{sec:layout}
Documents come in many different layouts.
Images, figures, number of columns, and similar aspects, add a layer of complexity to documents.
In the DLA stage, and when the model outputs the final result, to be accurate, the model must have some method to record and replicate the reading order.
A paper formatted with two columns, such as this, is meant to be read left column, then right column.
Unless otherwise instructed, an OCR model will take the first line from each column and treat them as one line. 

Some OCR models are intentionally made to only handle one layout, such as a specific tax form, or a job application for a specific company. 
A specialized OCR model, when applied to the layout it is made for yields higher accuracy.
Layout specific-OCR strategies are not directly applicable to other layouts, and can not be readily combined with other layout-specific models.

One related feature of document text, is line spacing. 
If lines of text are close together, or overlapping, the boxes drawn around the lines, will overlap.
This can also occur if the lines of text are not straight or are at an angle.
This results in the offending overlap being taken into consideration when identifying the otherwise unrelated text. 
%One related challenge to OCR accuracy, is curved lines of text. As seen in Figure~\ref{fig:stages}, the text on the source document was angled. In this figure you can also see that the DLA and TLD stages use rectangles to section off portions of text. By intentionally rotating the lines of text, the boxes are a tighter fit to the text, creating more consistency between the characters and increasing accuracy~\cite{Fateh:2024}.

\subsection{Visual Noise}
\label{sec:noise}

One big factor in the accuracy of OCR is the quality of the initial image. 
Marks on the page, or noise acquired when the image was captured, add additional complexity to the OCR process. 
In 2022, Thomas Hegghamer~\citep{Hegghammer:2022}, a historian, performed a bench-marking experiment to better understand how OCR models of the time were impacted by the types of noise he found in historical documents. 
Figure~\ref{fig:noise} shows the six different types of visual noise he studied.
To perform this experiment Hegghammer created a dataset based on 422 documents from two existing datasets.
Hegghammer kept a color version of each of the documents and made a copy in black and white.
To each of the versions, he then applied layers of each of the noise types, so each version had additional versions with zero, one, or two layers of noise.
In total, each starting document had 43 variations of each, for a total of 18,568 documents.

Hegghammer came to two conclusions about the impact of noise on the OCR model's accuracy.
First, documents with several layers of noise, produced OCR output with higher percentages of incorrect words, than those with less layers of noise.
Second, \textit{integrated noise}, noise that was built into the document, had a larger impact on accuracy than \textit{superimposed noise}.
In Figure~\ref{fig:noise}, Blur and Salt and Pepper are the two types of integrated noise.
\todo[inline]{give some numbers, maybe just some Tesseract ones?}

Integrated noise impact the ability to distinguish the boundaries and lines of characters.
Two additional ways that noise can reduce OCR accuracy, is by being mistaken for characters, or by covering up characters.
In instances like, Scribbles and Watermark, from Figure~\ref{fig:noise}, the noise is made up of text, but is frequently not an indented part of the output.
Because of the placement of Watermark, the superimposed character can be identified and inserted throughout the output.
Noise types like Ink Stain can partially and fully obscure text.
In these cases, a human reader is left to fill in the missing text, by guessing or attempting to counteract the stain.
Obscured text in OCR can result in missing text and disrupts the reading order.

The errors resulting from noise, can be addressed in two parts of the OCR process: pre-processing and post-processing.
The techniques which are used in pre-processing are sometimes included in the DLA stage. 
This can include rotating the full image, removing any borders, and sharpening or blurring regions of the image~\citep{Avyodri:2022}. \todo[inline]{mention post-proccessing more}

%In figure~\ref{fig:noise} all noise types, with the exception of Salt and Pepper, can obscure the original text.
%In these cases, part, or all, of a character is unidentifiable.
%Some OCR models include an additional post-processing step where the OCR output is checked for spelling errors.
%This can help address potential missing letters.
%This post-processing step comes at the cost of potentially overwriting correctly identified text from the source document, such as the author misspelling a word.
%If the affected area of the document impacts several lines of text, the challenge of addressing the noise becomes the same challenge presented for complex page layouts~\ref{sec:layout}.
%Some care needs to be taken to ensure a large section of non-text resulting from visual noise is not interpreted as an intentional feature of the layout.

\subsection{Writing Systems}
\label{sec:alphabet}

The most commonly used writing systems, by number of users worldwide: are Latin, Chinese, and then Arabic~\cite{Vaughan:2025}.
The majority of OCR models are trained to recognize characters from the Latin alphabet. 
% Most, but not all, documents from American institutions are written in English, a language based on the Latin alphabet.
As mentioned in Recognition~\ref{sec:Recognition}, OCR models are generally limited in what characters they can recognize, to ones they have been exposed to previously.
Additionally, the techniques used to identify Latin characters do not automatically extend to characters from other writing systems.
The current limitations in identifying a variety of text types is most easily seen in non-Latin language documents, but also apply for documents with a variety of fonts, or documents with handwritten text.

Because of its popularity, and how different it is from Latin, Arabic is a relevant example to discuss this weakness in OCR.
The Arabic alphabet has three main differences from Latin: the use of connected characters, the use of diacritics, and the reading order.

\begin{figure*}
  \includegraphics[width=0.5\textwidth]{arabic.png}
  \caption{The heading of the Latin Alphabet Wikipedia page, in Arabic.}
  \label{fig:alphabet}
\end{figure*}

\subsubsection{Connected Characters}

Arabic is a cursive language, where a character is connected to the characters before and after.
In a study by Fateh et al~\cite{Fateh:2024} on improving TLD accuracy for Persian text, they found, to better account for connected characters, the boxes drawn around each character must be larger.\todo[inline]{go back to}
% This paper explored the effects of changing the size of boxes when isolating characters in Arabic documents. They found that by increasing the size of the boxes, they had a higher accuracy. 
% When the size of boxes around characters increase, curved text becomes a larger problem~\citep{Fateh:2024}.

Connected characters often appear in handwritten documents, regardless of writing system. Arabic is notable here because it includes it, where printed Latin characters are not cursive.

\subsubsection{Diacritics}

A diacritic is a small graphic symbol added to a letter.
In Figure~\ref{fig:alphabet}, diacritics can be seen both above and below the main line of text.
Written Arabic does not include vowels, and instead relies on the reader to use context clues to place them.
Diacritics are especially important to the Arabic alphabet because they can be used to indicate the necessary vowel when the context is ambiguous~\cite{ArabicWiki:2025}.
These diacritics can be mistaken for visual noise.

Diacritics appear in other Latin-based languages such as German and Spanish, but to a lesser degree.

\subsubsection{Direction}

The Arabic writing system is read from right to left, where the Latin writing system is read from left to right.
During the DLA stage~\ref{sec:DLA}, and when the identified characters are assembled into the output, parts of the process must be reversed, to accommodate this.
For writing systems such as [] and [], where they are read top to bottom, the techniques used in OCR must be adjusted further, to change how the regions of text are drawn to begin with.
\todo[inline]{enter writing systems}

% Variations in fonts and alphabets introduce an added layer of complexity that need to be accounted for when performing the character recognition step. 
% In many cases, such as the Cirrilic "", it adds confusion, where the character is mis-classififed as the Latin "H".
% These models are not automatically applicable to other alphabets.  This inherently gives all other languages a bit of a disadvantage. \cite{Fateh:2024, Hegghammer:2022}

\section{Datasets}
\label{datasets}

%In an effort to better understand the impact of visual noise and the Arabic alphabet on popular OCR models, a historian named Thomas Hegghammer, performed a bench-marking experiment.
%To perform this experiment Hegghammer crafted a dataset called the ``Noisy OCR Dataset'', or NOD.
%The source documents in NOD come from two existing English and Arabic datasets.
%Each image was converted to black and white.
%Hegghamer chose six types of visual noise and applied every combination of noise, up to three types of noise per document, to both the color and black and white versions of the documents. The six types of noise are pictured in Figure~\ref{fig:noise}.
%Hegghammer found that of the types of noise he researched, noise that was built into the page had a larger effect on accuracy than noise that affected targeted areas.
%Built in noise, referred to blurred documents and documents with multicolored speckling applied. 
%Superimposed noise referred to things like scribbles, ink stains, and watermarks.\cite{Hegghammer:2022}

Thomas Hegghammer's effort to understand the effect of noise on OCR accuracy was limited in the variety of layouts present in the documents and in the types of noise.
``While not an exhaustive list of possible noise types, they represent several of the most common ones found in historical document scans.''
Even with this limitation, his study gives insight into the scale of impact noise had on the models he tested and gives insight into what noise has a more pronounced impact.
Thomas Hegghamer made contents of his dataset publicly available, along with the noise generator he made, and the output from each of the models evaluated, for each of the documents.
The documents in NOD, are about 26 GB when compressed and 193 GB uncompressed.

Fateh et al~\cite{Fateh:2024} look at new methods to improve accuracy of the TLD and DLA steps when applied to Persian text.
Persian is derived from the Arabic script.
In this paper, the authors discuss the use of separate datasets to test their proposed TLD and DLA methods.
This paper highlights several DLA-specific datasets which utilize newspapers and magazine collections to provide a variety of layouts.
When it came to testing their TLD method, they specifically note: ``TLD in complex scripts like Persian and Arabic presents unique challenges, and the availability of suitable standard datasets is limited. Unlike English or other widely studied languages, Persian and Arabic require specialized datasets and approaches to tackle text line extraction effectively.''
In total, this paper used three TLD datasets, and five DLA datasets.
Of the three TLD datasets, one of them was specifically created for this study.
The Official Iranian Newspapers (OIN) dataset, which is made of images of Iranian newspapers, was made to have rotated lines of text, regions with closely spaced lines, and to have a large amount of diacritics. 

% Some OCR models, notably Tesseract, have adapted to use machine learning to recognize text. Machine learning, in this case, works like matrix matching, discussed in Recognition~\ref{sec:Recognition}, with the advantage that the existing collection of known characters is updated as the model is used on more documents. This method is better suited to recognize a variety of fonts and alphabets, but is still limited in some capacity by its access to a variety of documents. Tesseract has also removed the Text Line Detection Step, instead of identifying text by individual characters, it uses full lines of text. This reduces errors introduced when segmenting the text and minimizes the steps needed to reconstruct the text to output it.\todo[inline]{cite}

\section{Conclusion}
\label{conclusion}

We know that there are existing weak spots within the field of OCR.
Through studies like Hegghamer's bench-marking experiment~\cite{Hegghammer:2022}, we know that these weaknesses can be identified and measured. 
Research like Fateh et al~\cite{Fateh:2024} shows us there is more progress to be made in TLD and DLA stage techniques.
By understanding the process of character recognition, we know the role of training documents, in increasing the number of characters an OCR model can be used to identify.

All of these findings were made possible because of specialized datasets.
Both Hegghamer and the Fateh research team utilized and built off of existing datasets.
Because of researchers prior, these studies exist. %Hegghammer and Thorat were able to do their research. 

While it may be ideal to create one dataset which contains some collection of documents which covers every one of the challenges from this paper perfectly, it is not realistic. 
%Because of variability and physical constraints an all-encompassing dataset is not realistic, so specialized datasets are made to fit this need. 
Due to the nature of layout and visual noise, it is inherently impossible to cover all scenarios. 
Covering every writing system is similarly difficult, but to a lesser degree.
As discussed in Datasets~\ref{datasets}, Hegghammer's dataset, which he admitted lacked layout variation and did not cover all noise types, is a total of around 193 GB.
This is a large amount of storage for a dataset which covers a mere fraction of the challenges presented.
Specialized datasets fill a niche needed for this technology, while working within storage and processing constraints. 

By increasing the number of datasets made to cover the current challenges to OCR, and by improving the tools used to make these datasets, we can increase the energy spent to actually address these problem areas.

The challenges identified in this paper are discussed in contexts where they perform notably poorly, but there are also areas where OCR performs well, but still has room for improvement.
Handwriting OCR, a subsection of OCR specifically for identifying handwritten text, also has impacted accuracy due to connected characters.
The techniques used to identify Arabic can also be applied to cursive English.
% se problem areas show the worst in OCR, but these weakness are visible in other applications of OCR, most notably, the techniques used to identify connected characters in Arabic, help to identify cursive English.

A 2022 literature review of OCR research papers found that several implementations and techniques for English scanned documents resulted in accuracy rates above 90\%.
In some cases, techniques employed resulted in accuracy rates as high as 98 and 99\%~\cite{Avyodri:2022}.
While this is impressive, when taken in the context of larger bodies of work, this still leaves a large amount of errors.
In cases like digital accessibility, we care a lot about truthful representation of the original document's text, so continuing to increase overall OCR accuracy is a priority.

While the specialized datasets introduced in this paper may seem unnecessary and irrelevant on first glance, they address the needs of real people and the research used to improve accuracy in those cases has a ripple effect on the larger field.%, has impacts on the larger field. 

%To compare the accuracy of OCR models, the models must be run on the same collection of documents, a \textit{dataset}.
%Hegghammer's findings do not work directly to increase OCR accuracy, but his findings support future research in this field.
%His paper very directly identifies specific noise types that had the largest impact on OCR accuracy.
%Hegghammer's dataset and the dataset used by Fateh et al were built off of existing datasets.
%By reducing the labor required to make datasets, it becomes easier to make specialized datasets to begin to cover the challenges identified in this paper.
%Hegghammer continued this cycle, by making his dataset and noise generator publicly available.
%These future datasets can then be used to both evaluate models and to train models.

%Specialize datasets are the key to addressing each of the challenges in this paper. 
%At this point we do not have one all-encompassing dataset which covers every possible document type. 
% If the goal of the model is to better handle the challenges identified in this paper, the dataset should include as many edge cases as possible. 
%Due to the nature of layout and visual noise, it is inherently impossible to cover all scenarios. 
%Covering every alphabet is similarly difficult, but to a lesser degree. 
%These specialized datasets fill a niche needed for this technology, while working within storage and processing constraints. 

%Serious consideration to storage constraints need to be had when making these data sets.
%Hegghammer recognizes in his paper that there is limited variation in the layouts of the Arabic documents he included.

%Improving accuracy is so important for applications such as digital document accessibility because and incorrectly identified character must be edited by hand.

%All that said, these individual datasets are an important step in progress towards improving OCR accuracy. The overall discussion of weaknesses in current OCR technology guide improvements to the technology. These specialized datasets made to address these challenges, while not all-encompassing, make space for developments in those specialized areas. Similarly, the techniques used to construct these datasets can be used to make more datasets which can cover more of these cases. By pushing the bounds of what OCR models can do, we can strengthen their current capabilities.

%While it's ideal to have one golden bench-marking set, that's hard (because of the reasons outlined above), so now we have specialized ones. 
%Honestly I don't have a full stance on if I think these specialized sets are good. Yes, they highlight weaknesses of general OCR models, and push the development of OCR further, but blind acceptance and promotion of them can neglect some of the specialties of OCR, like layout-specific models. Data sets also don't really encourage reducing time and resource complexity for models, which I would like to see.
%I am interested in this topic of OCR because of the role I think it could play in digital accessibility, but for that we would need to push for more free models with graphic user interfaces.\footnote{for context, Document AI and Textract are both paid models, and Tesseract only has 3rd party GUIs} 


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
Thank you to my adviser, Elena Machkasova, and to my proofreader, Skatje Myers.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{anderson_paper}

\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
