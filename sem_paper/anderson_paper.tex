%%
%% This is file `sample-sigplan.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigplan')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigplan.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\DocumentMetadata{
  lang=en,
  pdfversion=2.0,
  %pdfstandard=ua-2,
  %testphase={phase-III,firstaid,math,title}
  tagging=on,
  testphase={phase-III,firstaid,math,title}
  %tagging-setup={math/setup=mathml-SE}
}
\documentclass[sigplan,screen,nonacm]{acmart-tagged}
\usepackage{color}
\setlength {\marginparwidth }{2cm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen,review]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{Challenges of Optical Character Recognition}
\author{Orville ``El'' Anderson}
\email{and10393@umn.edu}
\affiliation{
  \institution{Division of Science and Mathematics 
	\\
        University of Minnesota, Morris
	}
  \city{Morris}
  \state{Minnesota}
  \country{USA}
  \postcode{56267}
}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Optical Character Recognition (OCR) is technology used to extract text from images.
OCR has a wide variety of uses, with one common application being to digitize scanned documents.
OCR has three main categories of challenges that reduce accuracy when applied to scanned documents, stemming from page layouts, the alphabet used, and visual noise.
By intentionally expanding the documents used to train modern OCR models, we can increase the range of capabilities of this technology.
This paper looks at some of the ways that OCR models have adapted to address these challenges, and at some examples of datasets which are made to cover some of these document variations.
\end{abstract}

\doclicenseThis

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{optical character recognition, scanned documents, layout, languages, visual noise, datasets}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{introduction}

Over time, American institutions have accumulated a tremendous amount of scanned documents. 
In April of 2024, the Department of Justice updated the Americans with Disabilities Act to include access to digital content such as scanned documents. 
Among other requirements, all scanned documents made publicly accessible by state and local governments must now be usable by a screen reader.

When a document is scanned, it becomes an image and looses all record of the content found on the original document. The first step to make a scanned document screen-readable is to recognize the text lost when the document was scanned. This process can be done manually, but that isn't well suited for large numbers of documents. Instead we look to \textit{Optical Character Recognition (OCR) models}, programs made to extract text from images.\todo[inline]{there's a little more to it, but generally some software that takes and input, applies all stages, then outputs something}

The Background~\ref{background} of this paper covers the three main steps in OCR. Challenges~\ref{challenges} looks at how layout, writing system, and visual noise impact OCR output accuracy. Results~\ref{results} section looks at two examples of datasets designed to address these challenges and looks at how the OCR model Tesseract has adapted to address some of these challenges. Conclusion~\ref{conclusion} discusses the importance of specialized datasets and their role in increasing accuracy of OCR.

\section{Background}
\label{background}

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{stages.png}
\caption{Stages of OCR pictured on a Persian Document\cite{Fateh:2024}.}
\Description{Some description}
\label{fig:stages}
\end{figure*}

The process of OCR starts with an existing scanned document. These documents can come in many file types, some common ones are .tiff, .pdf, and .jpg. 
% The first step in using OCR on a document is to acquire an image of the document. This can be done using a camera or a scanner. These images can be saved as a variety of formats, such as a .pdf, .jpg, or .png. 
This file is then input to an OCR model, where the model will go through the three stages of OCR. Figure~\ref{fig:stages} shows a document, with images, in Persian, going through each of the stages of OCR. The output of the model is the text identified from the document. This is generally returned as plain text, but can also include meta-information~\footnote{Modern OCR models can use this meta-information and text to construct new output formats.}.

As pictured in Figure~\ref{fig:stages}, the first stage of OCR is \textit{Document Layout Analysis} (DLA) which breaks down the page into sections of text. The second stage is \textit{Text Line Detection} (TLD), which then further breaks down the sections into individual lines of text, or into individual words. The final stage is Classification and Recognition which identifies the text and outputs it as a search-able text document.

\subsection{Document Layout Analysis}
\label{sec:DLA}
DLA is a general pre-processing step. The purpose is to identify what part of the image is text and what is not. This effectively draws a box around each paragraph and table on the page.
% This step addresses some of the complexity introduced when the image was made. This step can include rotating the full image and cropping out borders.
This step frequently outputs the result as a binary image, where each pixel is marked as a text or non-text pixel, to reduce computation costs.

An important step of DLA is preserving the reading order of the document, which is non-obvious for multi-column documents, documents with table, and other documents with more complex structure. Without this step, OCR models are effectively limited to simple single-column text inputs.

\subsection{Text Line Detection}
\label{sec:TLD}
TLD is the second step in the OCR process. TLD takes the blocks of text from the previous step and further breaks them down into lines, words, and then individual characters. The output of this step is each identified character in its own defined box of pixels.\todo[inline]{mention the pixel dimensions?}

A common technique used in this step includes rotating the individual lines of text to create a baseline. This can be seen in Figure~\ref{fig:stages}, where the text entering the TLD step is at an angle, but the output is rotated so each line is horizontal.
By creating this baseline and somewhat standardizing the characters, accuracy in defining characters is improved~\cite{Fateh:2024}.
% Two methods employed by Fateh et al\citep{Fateh:2024} used font size and correcting line rotation, to improve overall accuracy.
% After the individual characters have been identified, we go through and look at each of the boxes we have drawn around the characters and mark which pixels we think are text and which are not.

\subsection{Recognition}
\label{sec:Recognition}
The final step in the OCR process is referred to as Classification or Recognition.
This step takes the boxes of individual characters from the last step and tries to identify the character inside of them.

One of the foundational techniques used in character recognition is known by a variety of names, one being Matrix Matching. The technique uses templates of known characters. In this process, the unknown character is compared to all of the templates. The output of Matrix Matching is a list of the templates and how similar they were to the unknown character. The template that is the most similar, is chosen as the identity of the unknown character.

An increasingly popular technique for identifying characters uses Neural Networks. This technique is similar to Matrix Matching where the input is the unknown character, and the output is the full list of possible characters and a measure of confidence for each option. In Matrix Matching, each mismatched pixel has a uniform impact on the resulting similarity metric. In a Neural Network, each pixel and possible character outcome is given an associated weight. This means that the presence or absence of pixels can have a larger impact on the confidence in a certain character outcome. To compensate for the added complexity, a Neural Network will often include some method, an \textit{activation function}, which takes the input values and weights, and then scales them, so the resulting confidence measure is a percent between 0 and 100.

Neural Networks are popular because the weights add a level of flexibility when it comes to identifying characters. Neural networks are more accommodating to types of fonts and variations in character rotation and placement. Neural Networks can also include a condition where if the confidence is low for all of the possible outcome characters, the unknown character will be added to the dictionary of possible outcomes. This feature makes it easier to identify unknown characters, but it is important to know that Neural Networks, like Matrix Matching, are still limited in some capacity by the characters they have been exposed to previously. Neural Networks can be trained to identify a larger number of characters, by intentional exposure.

%Additionally Neural Networks use  an existing collection of known characters, and the character with the most similarity is chosen. \cite{Thorat:2022}
% Another popular technique, called feature extraction uses measurements like character height, width, and presence of loops to identify a character.
% There are a lot of variations in classification techniques, but the key thing to note is that all methods use reference material as a basis to classify characters~\cite{Thorat:2022}.
% The output of this recognition step is inherently limited to characters the model has been trained on.
% There are variations in classification techniques, including a subsection of OCR called Intelligent Character Recognition which uses machine learning, but all methods use some sort of training or reference material to classify.

\section{Challenges}
\label{challenges}
There are three main categories of issues that decrease accuracy in the OCR process. Each of these challenges tie back to the key concept that OCR models work best when applied to what they were made to recognize.

\subsection{Layout}
\label{sec:layout}
Documents come in many different layouts. Images, figures, number of columns, and similar aspects add a layer of complexity to documents. In the Document Layout Analysis step and when the model outputs the final result, to be accurate, the model must have some method to understand the reading order. 
% In the Document Layout Analysis step of OCR the model must know which parts of the page to ignore, but also to know what order the sections of text should go in.
A paper formatted with two columns, such as this, is meant to be read left column, then right column. Unless otherwise instructed, an OCR model will take the first line from each column and treat them as one line. 
% While understanding paragraph breaks and readign order is generally intuitive, this is something that must be intentionally taken into account when training OCR.

Some OCR models are intentionally made to only handle one layout, such as a specific tax form, or a job application for a specific company. A specialized OCR model, when applied to the layout it is made for, yields higher accuracy.
% This increase in accuracy is not applicable to layouts beyond the intended one, and is generally not combined with other layout-specific models.
Layout specific-OCR strategies are not directly applicable to other layouts, and can not be easily combined with other layout-specific models.\todo[inline]{they can be combined, but they require some human input to say what document it is.}

One related challenge to OCR accuracy is curved lines of text. As seen in Figure~\ref{fig:stages}, the text on the source document was angled. In this figure you can also see that the DLA and TLD stages use rectangles to section off portions of text. By intentionally rotating the lines of text, the boxes are a tighter fit to the text, creating more consistency between the characters and increasing accuracy~\cite{Fateh:2024}.

\subsection{Visual Noise}
\label{sec:noise}

\begin{figure}
\includegraphics[width=\linewidth]{noise.png}
\caption{Examples of visual noise from Hegghammer~\cite{Hegghammer:2022}: Salt and Pepper, Watermark, Scribbles, and Ink Stains}
\label{fig:noise}
\Description{A page out of a book in English with an image of a chair on the left side. There are four copies of the image with the labels, e - salt and pepper, f - watermark, g - scribbles, and h - ink stains. Version E has a layer of static over the page. Version f has large grey text which reads "Waterm" over the page. Version g has scribbles in various shapes over the text and images. Version h has a large black triangle covering the text and part of the image.}
\end{figure}

One big factor in the accuracy of OCR is the quality of the initial image. Marks on the physical document, damaged pages and low image resolution all add additional complexity to the process. There are two main ways that visual noise impacts OCR output, obscuring characters and adding characters.

In figure \ref{fig:noise} cases f, g, and h are examples of visual noise obscuring the original text. In these cases, part, or all, of a character is unidentifiable. Some OCR models include an additional post-processing step where the OCR output is checked for spelling errors. This can help address potential missing letters. This post-processing step comes at the cost of potentially overwriting correctly identified text from the source document, such as the author misspelling a word. 

Cases e and g from Figure~\ref{fig:noise} are examples of where the OCR may identify regions of non-text as text. If a section of visual noise is large enough, it can be identified as a character, frequently as punctuation.

\subsection{Writing Systems}
\label{sec:alphabet}
The most commonly used writing systems, by number of users worldwide, are Latin, Chinese, and then Arabic~\cite{Vaughan:2025}.
The majority of OCR models are trained to recognize characters from the Latin alphabet. 
Most, but not all, documents from American institutions are written in English, a language which uses the Latin alphabet.
As mentioned in Recognition~\ref{sec:Recognition}, OCR models are limited in what they can recognize, to the characters they were trained on.
The ability to recognize a Latin character does not automatically extend to characters from other writing systems.
This weakness in OCR models is most easily seen in non-Latin language documents, but can also be seen when using these models on documents with a variety of fonts, or documents with handwritten text.

The best writing system to highlight this weakness in OCR is Arabic.
The Arabic alphabet has two main features, that are not common in the Latin alphabet. The first is the use of connected characters, the second is the use of diacritics. Arabic is a cursive language, where a character is connected to the characters before and after it. A diacritic is a small graphic symbol added to a letter. Connected characters often appear in English handwritten documents and diacritics appear in other Latin-based alphabets, such as Spanish and German.

During the TLD step, to better account for connected characters, the boxes drawn around each character must be larger. When the size of boxes around characters increase, curved text becomes a larger problem~\citep{Fateh:2024}.

\begin{figure}
  \includegraphics[width=\linewidth]{arabic.png}
  \caption{Example of Arabic text, The heading of the Latin Alphabet Wikipedia page}
  \label{fig:alphabet}
\end{figure}

In figure \ref{fig:alphabet}, diacritics can be seen both above and below the main line of text. Written Arabic does not include vowels, and instead relies on the reader to use context clues to place them. Diacritics are especially important to the Arabic alphabet because they can be used to indicate the necessary vowel when the context is ambiguous~\cite{ArabicWiki:2025}. These diacritics can be mistaken for visual noise.

% As this paper\cite{Fateh:2024} says about Arabic, "Finally, character and TLD approaches cannot be universally applied to scripts with different languages. This challenge is particularly pronounced in languages such as Persian and Arabic, where text characters can take the form of connectors or non-connectors. Connector letters are affixed to both pre- and post-letters to form words, and diacritic marks are commonly used in Arabic textâ€”both of which can introduce complexity to TLD techniques." This paper explored the effects of changing the size of boxes when isolating characters in Arabic documents. They found that by increasing the size of the boxes, they had a higher accuracy. 

% Variations in fonts and alphabets introduce an added layer of complexity that need to be accounted for when performing the character recognition step. 
% In many cases, such as the Cirrilic "", it adds confusion, where the character is mis-classififed as the Latin "H".
% These models are not automatically applicable to other alphabets.  This inherently gives all other languages a bit of a disadvantage. Other alphabets also have nuances that make it harder to directly apply OCR to. A common example is Arabic. (there's dots) \cite{Fateh:2024, Hegghammer:2022}

\section{Results}
\label{results}
In an effort to better understand the impact of visual noise and the Arabic alphabet on popular OCR models Thomas Hegghammer performed a bench-marking experiment. Hegghammer made a dataset of English and Arabic documents artificial visual noise applied and used three popular OCR models on them. ``While not an exhaustive list of possible noise types, they represent several of the most common ones found in historical document scans.'' Hegghammer found that certain types of visual noise such as blur and salt and pepper specks, reduced output accuracy more than types like ink stains and watermarks.~\cite{Hegghammer:2022} While this doesn't directly provide a way to address these challenges, it highlights them and provides resources, such as the dataset and his noise generator, which can be used to train future models. 

Fateh et al~\cite{Fateh:2024} look at new methods to improve accuracy of the TLD and DLA steps when applied to Persian text. The Persian script derived from the Arabic script. In this paper, the authors discuss the use of separate datasets to test their proposed TLD and DLA methods. This paper highlights several DLA-specific datasets which utilize newspapers and magazine collections to provide a variety of layouts. When it came to testing their TLD method, they specifically note: ``TLD in complex scripts like Persian and Arabic presents unique challenges, and the availability of suitable standard datasets is limited. Unlike English or other widely studied languages, Persian and Arabic require specialized datasets and approaches to tackle text line extraction effectively.'' In response to the lack of a suitable TLD dataset, the paper introduces a specific Persian dataset.

Some OCR models, notably Tesseract, have adapted to use machine learning to recognize text. Machine learning, in this case, works like matrix matching, discussed in Recognition~\ref{sec:Recognition}, with the advantage that the existing collection of known characters is updated as the model is used on more documents. This method is better suited to recognize a variety of fonts and alphabets, but is still limited in some capacity by its access to a variety of documents. Tesseract has also removed the Text Line Detection Step, instead of identifying text by individual characters, it uses full lines of text. This reduces errors introduced when segmenting the text and minimizes the steps needed to reconstruct the text to output it.\todo[inline]{cite}

\section{Conclusion}
\label{conclusion}
To compare the accuracy of OCR models, the models must be run on the same collection of documents, a \textit{dataset}. If the goal of the model is to better handle the challenges identified in this paper, the dataset should include as many edge cases as possible. Due to the nature of layout and visual noise, it is inherently impossible to cover all scenarios. Covering every alphabet is similarly difficult, but to a lesser degree. 

Serious consideration to storage constraints need to be had when making these data sets. Hegghammer's "Noisy OCR Dataset" (NOD), consists of 422 original documents with 43 variations of each, for a total of 18,568 documents. NOD, is about 26 GB when compressed and 193 GB uncompressed. Hegghammer recognizes in his paper that there is limited variation in the layouts of the Arabic documents he included. \todo[inline]{horrible news, i think this might be more, one noise type for a color version and one for black and white...}

Improving accuracy is so important for applications such as digital document accessibility because and incorrectly identified character must be edited by hand.

All that said, these individual datasets are an important step in progress towards improving OCR accuracy. The overall discussion of weaknesses in current OCR technology guide improvements to the technology. These specialized datasets made to address these challenges, while not all-encompassing, make space for developments in those specialized areas. Similarly, the techniques used to construct these datasets can be used to make more datasets which can cover more of these cases. By pushing the bounds of what OCR models can do, we can strengthen their current capabilities.

%While it's ideal to have one golden bench-marking set, that's hard (because of the reasons outlined above), so now we have specialized ones. 
%Honestly I don't have a full stance on if I think these specialized sets are good. Yes, they highlight weaknesses of general OCR models, and push the development of OCR further, but blind acceptance and promotion of them can neglect some of the specialties of OCR, like layout-specific models. Data sets also don't really encourage reducing time and resource complexity for models, which I would like to see.
%I am interested in this topic of OCR because of the role I think it could play in digital accessibility, but for that we would need to push for more free models with graphic user interfaces.\footnote{for context, Document AI and Textract are both paid models, and Tesseract only has 3rd party GUIs} 


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
Thank you.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{anderson_paper}

\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
